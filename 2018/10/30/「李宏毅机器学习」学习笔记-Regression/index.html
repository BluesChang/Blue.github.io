<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>「李宏毅机器学习」学习笔记-Regression | BlueCode</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-76546459-1','auto');ga('send','pageview');
</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">「李宏毅机器学习」学习笔记-Regression</h1><a id="logo" href="/.">BlueCode</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/donate/"><i class="fa fa-cny"> 赞助</i></a><a href="/app/"><i class="fa fa-adn"> 软件</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">「李宏毅机器学习」学习笔记-Regression</h1><div class="post-meta">Oct 30, 2018<span> | </span><span class="category"><a href="/categories/机器学习/">机器学习</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" data-disqus-identifier="2018/10/30/「李宏毅机器学习」学习笔记-Regression/" href="/2018/10/30/「李宏毅机器学习」学习笔记-Regression/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Step-1：Model"><span class="toc-number">1.</span> <span class="toc-text">Step 1：Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Step-2：Goodness-of-Function"><span class="toc-number">2.</span> <span class="toc-text">Step 2：Goodness of Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Step-3：Best-Function"><span class="toc-number">3.</span> <span class="toc-text">Step 3：Best Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#公式推导"><span class="toc-number">4.</span> <span class="toc-text">公式推导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#How’s-the-results"><span class="toc-number">5.</span> <span class="toc-text">How’s the results?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Selecting-another-Model"><span class="toc-number">6.</span> <span class="toc-text">Selecting another Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Overfitting"><span class="toc-number">7.</span> <span class="toc-text">Overfitting</span></a></li></ol></div></div><div class="post-content"><p>本章课程<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/Regression.pdf" target="_blank" rel="noopener">PDF</a>，视频（<a href="https://www.youtube.com/watch?v=fegAeph9UaA&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&amp;index=3" target="_blank" rel="noopener">油管</a>或<a href="https://www.bilibili.com/video/av10590361/?p=3" target="_blank" rel="noopener">B站</a>）。</p>
<p><em>Regression</em>：Output a scalar。</p>
<p>Regression可以用于预测股票市场、无人驾驶、推荐系统，最重要的是可以预测宝可梦（我更习惯于叫神奇宝贝）的CP值，宝可梦将会在李老师课程中出现多次，当然，本节课就是用这个例子讲解。</p>
<h3 id="Step-1：Model"><a href="#Step-1：Model" class="headerlink" title="Step 1：Model"></a>Step 1：Model</h3><p>所谓的<em>Model</em>就是a set of function。在这个例子中，我们可以把function写成$ y = b + w \cdot x_{cp}  $，因为$b$跟$w$可以带入各种值，所以function有无穷多个。这个model我们称之为Linear model：$ y = b +\sum  w_i \cdot x_i  $ （$w_i$：weight，$b$：bias）。</p>
<h3 id="Step-2：Goodness-of-Function"><a href="#Step-2：Goodness-of-Function" class="headerlink" title="Step 2：Goodness of Function"></a>Step 2：Goodness of Function</h3><p>首先准备好<em>training data</em>。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq1grce5ej20oo0dagn1.jpg" alt="Training Data"></p>
<p>我们需要使用<em>Loss Function</em>衡量function的效果好坏。</p>
<p>Loss Function $L$: Input：a function，output：how bad it is</p>
<p>此时，可以如下定义Loss Function：</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq1lpf3lyj20i20a53zu.jpg" alt=""></p>
<p>将Loss Function可视化绘图，图上每个点就是一个function，图上蓝色代表loss value很小，红色代表loss value很大。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq1n41yj0j20oq0fy0v0.jpg" alt=""></p>
<h3 id="Step-3：Best-Function"><a href="#Step-3：Best-Function" class="headerlink" title="Step 3：Best Function"></a>Step 3：Best Function</h3><p><img src="https://ws1.sinaimg.cn/large/d1566b02ly1fwq1wv6wp9j20gz08v3zj.jpg" alt=""></p>
<p>可以用线性代数迅速求得解。还可以使用更general的方法，能够用在各种不同的task上，这个方法就是<em>Gradient Descent</em>。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq25xbrn6j20pw0fugob.jpg" alt=""></p>
<p><em>Gradient Descent</em>首先随机选取初始值，然后计算此处切线的斜率，斜率如果小于0，如图上小猴子位置，显然切线左边高右边低，此时应该把参数向右边移以减少Loss的值。如果斜率大于0，说明切线左边低右边高，应该把参数向左边移以减少Loss的值。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02ly1fwq2dnwr67j20q10esq5m.jpg" alt=""></p>
<p><em>learning rate</em>控制学习速度的快慢。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02ly1fwq2eo4uh9j20ps0fyad0.jpg" alt=""></p>
<p>有时候经过多次迭代会停在<em>local minima</em>，不是我们想要的<em>global minima</em>，所以不同的随机会得到不同的结果，很看人品。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq2r8asgoj20mz0e376f.jpg" alt=""></p>
<p>当有两个参数的时候，Gradient Descent的过程如上图所示。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq2w8ajh7j20m20egqcz.jpg" alt=""></p>
<p>Gradient Descent可视化过程如上图所示。颜色代表Loss的值，越深代表值越小。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq57tipgvj21e80w67lo.jpg" alt=""></p>
<p>参数如果走到一个<em>saddle point</em>（鞍点），Gradient Descent也会停下来。</p>
<h3 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h3><p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq5d0ydrsj215q0smtjl.jpg" alt=""></p>
<h3 id="How’s-the-results"><a href="#How’s-the-results" class="headerlink" title="How’s the results?"></a>How’s the results?</h3><p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq69rukdbj21dw0rq77h.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02ly1fwq6gmtutnj21es0swgq8.jpg" alt=""></p>
<p>可以看出，在Testing Data的error略大于Training Data的error。</p>
<h3 id="Selecting-another-Model"><a href="#Selecting-another-Model" class="headerlink" title="Selecting another Model"></a>Selecting another Model</h3><p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq6mmj4bzj21h413un75.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq6msr1t9j21h413un81.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq6mwe2wfj21gy13mqcr.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq6n1zh53j21h613wk1e.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02ly1fwq74spdn8j21ea11gaio.jpg" alt=""></p>
<p>越复杂的model在训练集上的error越小。</p>
<h3 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h3><p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq7ack82cj21fg0ycjxz.jpg" alt=""></p>
<p>一个比较复杂的model并不一定总是在testing data上给我们比较好的performance。</p>
<p><em>Overfitting</em>就是model在training data上表现很好，在testing data上表现不好。</p>
<p>应对Overfitting的方法：</p>
<ol>
<li><p>收集更多的数据。</p>
</li>
<li><p>以前的模型有一些隐藏的因素没有考虑进去。所以我们需要重新设计一番。</p>
<p>根据宝可梦种类采用不同的function<img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq7qjj1eoj21gm10a46d.jpg" alt=""></p>
<p><em>Regularization</em></p>
<p>我们使用新的Loss Function：<img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq8014ahrj213609imzr.jpg" alt=""></p>
<p>使用这个Loss Function找到的最好的function不止可以让$L$最小，同时还要是$w$很小的function。$w$很小的Function意味着Function是<em>smooth</em>（平滑）的。</p>
<p>我们相信在多数的状况下，smoother function更像是正确的function。</p>
<p>$\lambda$设置的大一些，找到的function就会越smooth。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fwq8o7qfl4j21io0w0qok.jpg" alt=""></p>
<p>从上图中可以发现，$\lambda$越大，Training Error越大。</p>
<p>我们希望选到smooth function，但是不要too smooth。</p>
<p>选择$\lambda$，获得最好的model。</p>
</li>
</ol>
<p><em>如果本博文对您有帮助，可以<a href="https://blueschang.github.io/donate/">赞助</a>支持一波博主~</em></p>
</div><div class="recommended_posts"><h3>推荐阅读</h3><li><a href="https://blueschang.github.io/2018/10/31/「李宏毅机器学习」学习笔记-Gradient Descent/" target="_blank">「李宏毅机器学习」学习笔记-Gradient Descent</a></li><li><a href="https://blueschang.github.io/2018/10/30/「李宏毅机器学习」学习笔记-Where does the error come from/" target="_blank">「李宏毅机器学习」学习笔记-Where does the error come from?</a></li><li><a href="https://blueschang.github.io/2018/10/29/「李宏毅机器学习」学习笔记-Introduction of Machine Learning/" target="_blank">「李宏毅机器学习」学习笔记-Introduction of Machine Learning</a></li><li><a href="https://blueschang.github.io/2018/10/29/李宏毅机器学习-笔记传送门/" target="_blank">「李宏毅机器学习」学习笔记传送门</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>BluesChang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2018/10/30/「李宏毅机器学习」学习笔记-Regression/">https://blueschang.github.io/2018/10/30/「李宏毅机器学习」学习笔记-Regression/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a> 许可协议。转载请注明出处！</li></ul></div><br><div class="tags"><a href="/tags/李宏毅-机器学习/">李宏毅 机器学习</a></div><div class="post-nav"><a class="pre" href="/2018/10/30/「李宏毅机器学习」学习笔记-Where does the error come from/">「李宏毅机器学习」学习笔记-Where does the error come from?</a><a class="next" href="/2018/10/29/「李宏毅机器学习」学习笔记-Introduction of Machine Learning/">「李宏毅机器学习」学习笔记-Introduction of Machine Learning</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://blueschang.github.io/2018/10/30/「李宏毅机器学习」学习笔记-Regression/';
    this.page.identifier = '2018/10/30/「李宏毅机器学习」学习笔记-Regression/';
    this.page.title = '「李宏毅机器学习」学习笔记-Regression';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//bluecode-1.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//bluecode-1.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://bluecode-1.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://blueschang.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Hexo-Github-博客/" style="font-size: 15px;">Hexo Github 博客</a> <a href="/tags/李宏毅-机器学习/" style="font-size: 15px;">李宏毅 机器学习</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/11/08/「李宏毅机器学习」学习笔记-“Hello world” of deep learning/">「李宏毅机器学习」学习笔记-“Hello world” of deep learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/08/「李宏毅机器学习」学习笔记-Backpropagation/">「李宏毅机器学习」学习笔记-Backpropagation</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/07/「李宏毅机器学习」学习笔记-Brief Introduction of Deep Learning/">「李宏毅机器学习」学习笔记-Brief Introduction of Deep Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/03/「李宏毅机器学习」学习笔记-Logistic Regression/">「李宏毅机器学习」学习笔记-Logistic Regression</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/02/「李宏毅机器学习」学习笔记-Classification/">「李宏毅机器学习」学习笔记-Classification：Probabilistic Generative Model</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/31/「李宏毅机器学习」学习笔记-Gradient Descent/">「李宏毅机器学习」学习笔记-Gradient Descent</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/30/「李宏毅机器学习」学习笔记-Where does the error come from/">「李宏毅机器学习」学习笔记-Where does the error come from?</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/30/「李宏毅机器学习」学习笔记-Regression/">「李宏毅机器学习」学习笔记-Regression</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/29/「李宏毅机器学习」学习笔记-Introduction of Machine Learning/">「李宏毅机器学习」学习笔记-Introduction of Machine Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/29/李宏毅机器学习-笔记传送门/">「李宏毅机器学习」学习笔记传送门</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://toaco.github.io/" title="Jeffrey‘s Blog" target="_blank">Jeffrey‘s Blog</a><ul></ul><a href="https://coder-wen.github.io/" title="coderwen的踩坑日记" target="_blank">coderwen的踩坑日记</a><ul></ul><a href="https://me.csdn.net/taoyafan" title="taoyafan" target="_blank">taoyafan</a><ul></ul><a href="https://smilexnan.github.io/" title="SmileCode" target="_blank">SmileCode</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">BlueCode.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" color="0,0,0" opacity="0.5" zIndex="-2" count="50" src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>