<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Auto-encoder | BlueCode</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-76546459-1','auto');ga('send','pageview');
</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Auto-encoder</h1><a id="logo" href="/.">BlueCode</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/donate/"><i class="fa fa-cny"> 赞助</i></a><a href="/app/"><i class="fa fa-adn"> 软件</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Auto-encoder</h1><div class="post-meta">Dec 26, 2018<span> | </span><span class="category"><a href="/categories/机器学习/">机器学习</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" data-disqus-identifier="2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Auto-encoder/" href="/2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Auto-encoder/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Deep-Auto-encoder"><span class="toc-number">1.</span> <span class="toc-text">Deep Auto-encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Auto-encoder-–-Text-Retrieval"><span class="toc-number">2.</span> <span class="toc-text">Auto-encoder – Text Retrieval</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Auto-encoder-–-Similar-Image-Search"><span class="toc-number">3.</span> <span class="toc-text">Auto-encoder – Similar Image Search</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Auto-encoder-–-Pre-training-DNN"><span class="toc-number">4.</span> <span class="toc-text">Auto-encoder – Pre-training DNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#De-noising-auto-encoder"><span class="toc-number">5.</span> <span class="toc-text">De-noising auto-encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Auto-encoder-for-CNN"><span class="toc-number">6.</span> <span class="toc-text">Auto-encoder for CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Unpooling"><span class="toc-number">6.1.</span> <span class="toc-text">Unpooling</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Deconvolution"><span class="toc-number">6.2.</span> <span class="toc-text">Deconvolution</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Use-decoder-to-generate-something"><span class="toc-number">7.</span> <span class="toc-text">Use decoder to generate something</span></a></li></ol></div></div><div class="post-content"><p>本章课程<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/VAE%20%28v5%29.pdf" target="_blank" rel="noopener">PDF</a>，视频（<a href="https://www.youtube.com/watch?v=YNUek8ioAJk&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&amp;index=26" target="_blank" rel="noopener">油管</a>或<a href="https://www.bilibili.com/video/av10590361/?p=28" target="_blank" rel="noopener">B站</a>）。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyjww46ws6j20n50arjt8.jpg" alt=""></p>
<p>训练一个encoder，通常有input，但是不知道code是什么，没有办法learn。训练一个decoder，知道output是什么，但没有input，也没有办法learn。可以把encoder、decoder接起来一起learn。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyjx1b7p68j20ne0gpwgf.jpg" alt=""></p>
<p>PCA做的事情其实类似。中间hidden layer又叫Bottleneck layer，因为code往往比input的维数小得多，看起来就很窄。hidden layer的output就是我们要找的code。</p>
<h3 id="Deep-Auto-encoder"><a href="#Deep-Auto-encoder" class="headerlink" title="Deep Auto-encoder"></a>Deep Auto-encoder</h3><p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyjx97tofaj20n10dstar.jpg" alt=""></p>
<p>中间特别窄的layer，这个layer的ouput就是code。从input到bottleneck的部分是encoder，从bottleneck到output部分是decoder。在训练时候可以保证参数是左右对称的，这样可以减少参数，防止overfitting。但是对称是不必要的，直接用BP训练也可以。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyjxemfsj9j20pi0ekwgx.jpg" alt=""></p>
<p>明显看出Deep Auto-encoder的output比PCA好很多。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02ly1fyjxg6ii8ej20os0jfqc7.jpg" alt=""></p>
<p>用PCA降成2维可视化后，会发现数字是混在一起的。用Deep Auto-encoder降维后，数字是分开的。</p>
<h3 id="Auto-encoder-–-Text-Retrieval"><a href="#Auto-encoder-–-Text-Retrieval" class="headerlink" title="Auto-encoder – Text Retrieval"></a>Auto-encoder – Text Retrieval</h3><p>把文章变成一个code，得到vector space model，把查询词汇也变成一个vector，两者之间做点积进行查询。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyjxi1p548j20m50e876e.jpg" alt=""></p>
<p>一种方法是词袋，但是这种方法中每个词都独立，不能知道语意。 </p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyjxi1rqhlj20oq0fitg2.jpg" alt=""></p>
<p>用Auto-encoder可以把语意考虑进来。词袋经过auto-encoder之后得到code。图中不同颜色代表文档的不同种类。用LSA的效果就不太好。</p>
<h3 id="Auto-encoder-–-Similar-Image-Search"><a href="#Auto-encoder-–-Similar-Image-Search" class="headerlink" title="Auto-encoder – Similar Image Search"></a>Auto-encoder – Similar Image Search</h3><p>最简单的方法是基于像素计算图片之间相似度，往往得到的结果会比较差。比如，会得到迈克尔杰克逊的照片比较像马蹄铁。而把image经过auto-encoder变成code，基于code计算相似度，结果就会好很多。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyjxqhclcyj20mp0k9qhf.jpg" alt=""></p>
<h3 id="Auto-encoder-–-Pre-training-DNN"><a href="#Auto-encoder-–-Pre-training-DNN" class="headerlink" title="Auto-encoder – Pre-training DNN"></a>Auto-encoder – Pre-training DNN</h3><p>为了在训练NN时找到一组好的初始值，可以用Auto-encoder做预训练。</p>
<p>预训练过程如图所示：</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02ly1fyjxwz26wsj20nd0e63zn.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/d1566b02ly1fyjxwz29ghj20np0bwmy3.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/d1566b02ly1fyjxwz2drej20np0c6dgy.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/d1566b02ly1fyjxwz4t09j20oc0ezdhm.jpg" alt=""></p>
<p>在训练第一个auto-encoder时，由于hidden layer的维数大于input维数，要加很强的正则项，e.g.对1000维的output做L1正则化（希望output稀疏）。否则hidden layer可能直接记住input，没有learn到任何东西。 在训练第二个auto-encoder时，要把database中所有digit都变成1000维vector。 以此类推，最后随机初始化输出层之前的权重。然后用BP做fine-tune。 </p>
<p>之前在训练较深的Neural Network时要用到预训练，但是现在没必要了，因为训练技术进步了。 但在有大量unlabelled data、少量labelled data时仍需要预训练，可以先用大量unlabelled data先把W1,W2,W3先learn好，最后的labelled data只需稍微调整weight就好。</p>
<h3 id="De-noising-auto-encoder"><a href="#De-noising-auto-encoder" class="headerlink" title="De-noising auto-encoder"></a>De-noising auto-encoder</h3><p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyjy1nw9yrj20ks0cyta5.jpg" alt=""></p>
<p>对输入的数据增加noise，然后再使经过auto-encoder的输出尽可能与没有noise的原数据一样。这样learn出来的结果更具有鲁棒性。 </p>
<h3 id="Auto-encoder-for-CNN"><a href="#Auto-encoder-for-CNN" class="headerlink" title="Auto-encoder for CNN"></a>Auto-encoder for CNN</h3><p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyjy8kcqm9j20f50hsad0.jpg" alt=""></p>
<h4 id="Unpooling"><a href="#Unpooling" class="headerlink" title="Unpooling"></a>Unpooling</h4><p>在pooling的时候记得从哪里取的值，unpooling的时候在对应位置恢复，其余位置补0。Keras中unpooling是直接把pooling后的每个值直接复制4份。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyjy8kjppcj20p40j0qbx.jpg" alt=""></p>
<h4 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h4><p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyjy8kc7f8j20nu0jagod.jpg" alt=""></p>
<h3 id="Use-decoder-to-generate-something"><a href="#Use-decoder-to-generate-something" class="headerlink" title="Use decoder to generate something"></a>Use decoder to generate something</h3><p>把图片从784维降到2维，根据code的分布选择红框，在红框里等间隔采样，作为decoder的input，生成图片。 选取红框的时候需要提前知道code的分布，才能知道哪里采样出来比较可能是image，假如红框选错了位置，选在右下方，就可能得不到数字。提前分析二维code分布比较麻烦，为了确保采样的都能是image，可以在code上加正则化。 在训练时，对code加L2正则化，这样code比较接近0。采样时在0附近采样，这样采样得到的就比较有可能都对应数字。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyjyfup3klj20qd0imn6b.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyjyfuqeg7j20o20csteg.jpg" alt=""></p>
<p><em>如果本博文对您有帮助，可以<a href="https://blueschang.github.io/donate/">赞助</a>支持一波博主~</em></p>
</div><div class="recommended_posts"><h3>推荐阅读</h3><li><a href="https://blueschang.github.io/2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model Part II/" target="_blank">「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model (Part II)</a></li><li><a href="https://blueschang.github.io/2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model Part I/" target="_blank">「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model (Part I)</a></li><li><a href="https://blueschang.github.io/2018/12/25/「李宏毅机器学习」学习笔记-Unsupervised Learning - Neighbor Embedding/" target="_blank">「李宏毅机器学习」学习笔记-Unsupervised Learning - Neighbor Embedding</a></li><li><a href="https://blueschang.github.io/2018/12/25/「李宏毅机器学习」学习笔记-Unsupervised Learning - Word Embedding/" target="_blank">「李宏毅机器学习」学习笔记-Unsupervised Learning - Word Embedding</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>BluesChang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Auto-encoder/">https://blueschang.github.io/2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Auto-encoder/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a> 许可协议。转载请注明出处！</li></ul></div><br><div class="tags"><a href="/tags/李宏毅-机器学习/">李宏毅 机器学习</a></div><div class="post-nav"><a class="pre" href="/2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model Part I/">「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model (Part I)</a><a class="next" href="/2018/12/25/「李宏毅机器学习」学习笔记-Unsupervised Learning - Neighbor Embedding/">「李宏毅机器学习」学习笔记-Unsupervised Learning - Neighbor Embedding</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://blueschang.github.io/2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Auto-encoder/';
    this.page.identifier = '2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Auto-encoder/';
    this.page.title = '「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Auto-encoder';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//bluecode-1.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//bluecode-1.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://bluecode-1.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://blueschang.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Hexo-Github-博客/" style="font-size: 15px;">Hexo Github 博客</a> <a href="/tags/李宏毅-机器学习/" style="font-size: 15px;">李宏毅 机器学习</a> <a href="/tags/剑指Offer-Python-Algorithm/" style="font-size: 15px;">剑指Offer Python Algorithm</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/06/03/「剑指Offer（第二版）」Python3实现/">「剑指Offer（第二版）」Python3实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/28/「李宏毅机器学习」学习笔记-Unsupervised Learning - Support Vector Machine/">「李宏毅机器学习」学习笔记-Support Vector Machine (SVM)</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/27/「李宏毅机器学习」学习笔记-Unsupervised Learning - Transfer Learning/">「李宏毅机器学习」学习笔记-Transfer Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model Part II/">「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model (Part II)</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model Part I/">「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model (Part I)</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Auto-encoder/">「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Auto-encoder</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/25/「李宏毅机器学习」学习笔记-Unsupervised Learning - Neighbor Embedding/">「李宏毅机器学习」学习笔记-Unsupervised Learning - Neighbor Embedding</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/25/「李宏毅机器学习」学习笔记-Unsupervised Learning - Word Embedding/">「李宏毅机器学习」学习笔记-Unsupervised Learning - Word Embedding</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/24/「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods/">「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/20/「李宏毅机器学习」学习笔记-Semi-supervised/">「李宏毅机器学习」学习笔记-Semi-supervised</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://toaco.github.io/" title="Jeffrey‘s Blog" target="_blank">Jeffrey‘s Blog</a><ul></ul><a href="https://coder-wen.github.io/" title="coderwen的踩坑日记" target="_blank">coderwen的踩坑日记</a><ul></ul><a href="https://me.csdn.net/taoyafan" title="taoyafan" target="_blank">taoyafan</a><ul></ul><a href="https://smilexnan.github.io/" title="SmileCode" target="_blank">SmileCode</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">BlueCode.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>