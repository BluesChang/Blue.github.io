<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>「李宏毅机器学习」学习笔记-Support Vector Machine (SVM) | BlueCode</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-76546459-1','auto');ga('send','pageview');
</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">「李宏毅机器学习」学习笔记-Support Vector Machine (SVM)</h1><a id="logo" href="/.">BlueCode</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/donate/"><i class="fa fa-cny"> 赞助</i></a><a href="/app/"><i class="fa fa-adn"> 软件</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">「李宏毅机器学习」学习笔记-Support Vector Machine (SVM)</h1><div class="post-meta">Dec 28, 2018<span> | </span><span class="category"><a href="/categories/机器学习/">机器学习</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" data-disqus-identifier="2018/12/28/「李宏毅机器学习」学习笔记-Unsupervised Learning - Support Vector Machine/" href="/2018/12/28/「李宏毅机器学习」学习笔记-Unsupervised Learning - Support Vector Machine/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hinge-Loss"><span class="toc-number">1.</span> <span class="toc-text">Hinge Loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear-SVM"><span class="toc-number">2.</span> <span class="toc-text">Linear SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Linear-SVM-–-gradient-descent"><span class="toc-number">2.1.</span> <span class="toc-text">Linear SVM – gradient descent</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Linear-SVM-–-another-formulation"><span class="toc-number">2.2.</span> <span class="toc-text">Linear SVM – another formulation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kernel-Method"><span class="toc-number">3.</span> <span class="toc-text">Kernel Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Dual-Representation"><span class="toc-number">3.1.</span> <span class="toc-text">Dual Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kernel-Trick"><span class="toc-number">3.2.</span> <span class="toc-text">Kernel Trick</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SVM-related-methods"><span class="toc-number">3.3.</span> <span class="toc-text">SVM related methods</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Deep-Learning-vs-SVM"><span class="toc-number">4.</span> <span class="toc-text">Deep Learning vs SVM</span></a></li></ol></div></div><div class="post-content"><p>本章课程<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/SVM%20%28v5%29.pdf" target="_blank" rel="noopener">PDF</a>，视频（<a href="https://www.youtube.com/watch?v=QSEPStBgwRQ&amp;index=29&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49" target="_blank" rel="noopener">油管</a>或<a href="https://www.bilibili.com/video/av10590361/?p=31" target="_blank" rel="noopener">B站</a>）。</p>
<p>SVM有两大特色：Hinge Loss和Kernel Method。</p>
<p><img src="https://s2.ax1x.com/2019/06/05/VUEh36.png" alt=""></p>
<h3 id="Hinge-Loss"><a href="#Hinge-Loss" class="headerlink" title="Hinge Loss"></a>Hinge Loss</h3><p><img src="https://s2.ax1x.com/2019/06/05/VUEgE9.png" alt=""></p>
<p>损失函数使用$\delta()$函数计算时，$\delta()$函数不可微分，无法使用Gradient Descent求解。可以使用近似的其他函数代替$\delta()$函数。</p>
<p><img src="https://s2.ax1x.com/2019/06/05/VUE2NR.png" alt=""></p>
<p>图中横轴是$\hat{y}f(x)$ ,希望$\hat{y}=+1$时，$f(x)$越正越好，$\hat{y}=−1$时，$f(x)$越负越好。即$\hat{y}f(x)$越大，损失函数越小。Ideal loss是两者同向为0，反向为1。可微分的损失函数可以自己选。</p>
<p><img src="https://s2.ax1x.com/2019/06/05/VUEf9x.png" alt=""></p>
<p>红色线是Square Loss，是不合理的，$\hat{y}f(x)$很大时，损失函数很大。</p>
<p><img src="https://s2.ax1x.com/2019/06/05/VUER41.png" alt=""></p>
<p>蓝色线是Sigmoid + Square Loss。对$l(f(x^n)，\hat{y})$ , 代入$y^n=±1$，可以理解公式的合理性。逻辑回归时不会用square loss，因为它的performance不好，实际用cross entropy。</p>
<p><img src="https://s2.ax1x.com/2019/06/05/VUE5jO.png" alt=""></p>
<p>绿色线是Sigmoid + cross entropy。指的是$\sigma(f(x)),1−\sigma(f(x))$这个分布，与ground truth这一分布之间的cross entropy，这是最小化的目标。这时的损失函数$l(f(x^n),\hat{y}^n)$是合理的，这里可以除以ln2，就可以变成ideal loss的up-bound，通过最小化ideal loss的up-bound来最小化ideal loss。比较绿线和蓝线，可知道为什么在逻辑回归时要用cross entropy作为损失函数，而不用square loss：$\hat{y}f(x)$从-2变到-1时，蓝色线变化很小，绿色线变化很大；$\hat{y}f(x)$很负时，应该朝正方向调整，但对蓝色线来讲调整没有大的回报，而对绿色线调整有较大的回报。所以用逻辑回归的时候，经常使用cross entropy。</p>
<p><img src="https://s2.ax1x.com/2019/06/05/VUE4gK.png" alt=""></p>
<p>紫色线是Hinge Loss。损失函数$l(f(x^n),\hat{y}^n)$中的“1”可以使曲线过(0,1)(1,0)点，才会是ideal loss的一个type的upper bound, 可以通过最小化hinge loss来得到最小化ideal loss的效果。Hinge loss是一个”及格就好“的函数，只要值打过margin，它就不会想要做得更好。Hinge Loss与Sigmoid + cross entropy相比，区别在于对待已经做好的样本的态度：当$\hat{y}f(x)$从1变成2时，前者loss不降，及格就好；后者loss下降，还想要更好。在实践中其实相差不多，Hinge Loss可能会微胜Sigmoid + cross entropy。Hinge loss的好处是不害怕outlier，学习出的结果比较具有鲁棒性，下面讲kernel时会明显看到这一点。</p>
<h3 id="Linear-SVM"><a href="#Linear-SVM" class="headerlink" title="Linear SVM"></a>Linear SVM</h3><p> <img src="https://s2.ax1x.com/2019/06/05/VUEouD.png" alt=""></p>
<p>Linear SVM的function是linear的，如图中公式所示，当$f(x)&gt;0$时，x属于class 1, 当$f(x)&lt;0$时，x属于class 2。 </p>
<p>Linear SVM的loss function是hinge loss + 正则项，因为hinge loss和正则项都是convex的，所以loss function整体也是convex的。曲线有棱棱角角也是可微分的。 可以用Gradient Descent求解。</p>
<p>Linear SVM与Logistic regression 区别只在于损失函数不同，前者的损失函数是hinge loss，后者的损失函数是cross entropy。</p>
<h4 id="Linear-SVM-–-gradient-descent"><a href="#Linear-SVM-–-gradient-descent" class="headerlink" title="Linear SVM – gradient descent"></a>Linear SVM – gradient descent</h4><p><img src="https://s2.ax1x.com/2019/06/05/VUETDe.png" alt=""></p>
<p>SVM可以用Gradient Descent来解，公式推导如上图。</p>
<h4 id="Linear-SVM-–-another-formulation"><a href="#Linear-SVM-–-another-formulation" class="headerlink" title="Linear SVM – another formulation"></a>Linear SVM – another formulation</h4><p><img src="https://s2.ax1x.com/2019/06/05/VUE7HH.png" alt=""></p>
<p>将$l(f(x^n)，\hat{y})$记作$\varepsilon^n$。 </p>
<p>上下两个方框本是不等价的（上面可以推出下面，但是下面不能推出上面），但是加上“Minimizing loss function $L$”之后二者就等价了。下面就是常见的SVM的约束，$\varepsilon^n$是松弛因子，$\varepsilon^n$不能是负的，否则就不是松弛的。这是一个二次规划问题，可代入QP solver求解或用gradient descent求解。</p>
<h3 id="Kernel-Method"><a href="#Kernel-Method" class="headerlink" title="Kernel Method"></a>Kernel Method</h3><h4 id="Dual-Representation"><a href="#Dual-Representation" class="headerlink" title="Dual Representation"></a>Dual Representation</h4><p><img src="https://s2.ax1x.com/2019/06/05/VUV9bQ.png" alt=""></p>
<p>最小化损失函数的权重参数$w^∗$可以表示为数据点$x^n$的线性组合。一般用拉格朗日乘子法解释这一结论，这里从另外的角度解释：之前说过，Linear SVM可以用gradient descent来更新参数，根据前面得到的公式，每次更新权重都加上$x^n$的线性组合，那么如果$w$初始化为0向量的话，得到的$w^∗$就是$x^n$的线性组合。其中的权重$c^n(w)$是损失函数$l(f(x^n)，\hat{y})$对$f(x^n)$的偏导数，由于损失函数采用的是Hinge Loss，所以有的$c^n(w)$就是0，可能会有数据点对应的$\alpha^<em>$等于0，从而$\alpha^</em>_n$是sparse的，具有非零$\alpha^*_n$的数据点$x^n$是支持向量。这样的好处是模型比较鲁棒，不是支持向量的数据点，就算去掉也不会对结果有影响，奇怪的outlier只要不是支持向量，就不会对结果有影响。Logistic regression用cross entropy作损失函数，它在更新参数时的权重就不sparse，所以每笔data都对结果有影响。</p>
<p><img src="https://s2.ax1x.com/2019/06/05/VUVSKS.png" alt=""><br><img src="https://s2.ax1x.com/2019/06/05/VUVpDg.png" alt=""></p>
<p>把$w$写成$x^n$的线性组合，最大的好处是可以使用Kernel trick。根据$w=Xα$，$f(x)$可写为$f(x)=Σ_n\alpha_n(x^n⋅x)$，由于用的损失函数是Hinge Loss, 所以$\alpha_n$是sparse的，只需要算支持向量与数据点$x$之间的内积即可。 可以把内积$(x^n⋅x)$(写作Kernel function $K(x^n,x)$。这样，整体损失函数可以改写为图中的式子，我们不需要知道$x$，只需要知道Kernel function $K(x^n,x)$即可，这就叫Kernel trick。Kernel trick不只可用于SVM，也可用于logistic regression, linear regression等。（图中”project”应改为”product”）</p>
<h4 id="Kernel-Trick"><a href="#Kernel-Trick" class="headerlink" title="Kernel Trick"></a>Kernel Trick</h4><p><img src="https://s2.ax1x.com/2019/06/05/VUVPEj.png" alt=""><br><img src="https://s2.ax1x.com/2019/06/05/VUVZvT.png" alt=""></p>
<p>直接计算核函数有时候会比“特征变换+计算内积”快。</p>
<p><img src="https://s2.ax1x.com/2019/06/05/VUViUs.png" alt=""></p>
<p>$x$与$z$越像，则$K(x,z)$越大。它是两个无穷维特征向量的内积。将核函数展开并使用泰勒级数，可见核函数是无穷项之和，每项都可写成内积形式，将与$x$，$z$有关的向量分别串起来，得到两个无穷维的向量，这两个向量的内积就是RBF Kernel。由于使用了无穷维的特征，在用RBF Kernel时要小心overfitting,可能在training data 上得到很好的performance, 而在testing data 上得到很糟的performance。 </p>
<p><img src="https://s2.ax1x.com/2019/06/05/VUVV2V.png" alt=""></p>
<p>Sigmoid Kernel可看做一个单隐层网络，neuron个数就是支持向量的个数，neuron权重就是某一笔data。</p>
<p><img src="https://s2.ax1x.com/2019/06/05/VUVF5n.png" alt=""></p>
<h4 id="SVM-related-methods"><a href="#SVM-related-methods" class="headerlink" title="SVM related methods"></a>SVM related methods</h4><p><img src="https://s2.ax1x.com/2019/06/05/VUVE80.png" alt=""></p>
<h3 id="Deep-Learning-vs-SVM"><a href="#Deep-Learning-vs-SVM" class="headerlink" title="Deep Learning vs SVM"></a>Deep Learning vs SVM</h3><p><img src="https://s2.ax1x.com/2019/06/05/VUVACq.png" alt=""></p>
<p>Deep Learning的前几个layer可以看成Feature transformation，最后一个layer可以看作Linear Classifier。SVM先应用一个Kernel Function，把feature转到高维上面，在高维空间上面应用Linear Classifier，在SVM里面一般Linear Classifier会用Hinge Loss。</p>
<p>事实上，SVM的kernel是learnable，但没有办法learn的像Deep Learning那么多。可以有好几个不同的kernel，然后把不同kernel连接起来，它们中间的权重是可以学出来的。当只有一个kernel的时候，SVM就好像只有一个隐藏层的Neural Network，当把kernel做线性组合的时候，就好像一个有两个layer的Neural Network。</p>
<p><em>如果本博文对您有帮助，可以<a href="https://blueschang.github.io/donate/">赞助</a>支持一波博主~</em></p>
</div><div class="recommended_posts"><h3>推荐阅读</h3><li><a href="https://blueschang.github.io/2019/06/03/「剑指Offer（第二版）」Python3实现/" target="_blank">「剑指Offer（第二版）」Python3实现</a></li><li><a href="https://blueschang.github.io/2018/12/27/「李宏毅机器学习」学习笔记-Unsupervised Learning - Transfer Learning/" target="_blank">「李宏毅机器学习」学习笔记-Transfer Learning</a></li><li><a href="https://blueschang.github.io/2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model Part II/" target="_blank">「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model (Part II)</a></li><li><a href="https://blueschang.github.io/2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model Part I/" target="_blank">「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model (Part I)</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>BluesChang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2018/12/28/「李宏毅机器学习」学习笔记-Unsupervised Learning - Support Vector Machine/">https://blueschang.github.io/2018/12/28/「李宏毅机器学习」学习笔记-Unsupervised Learning - Support Vector Machine/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a> 许可协议。转载请注明出处！</li></ul></div><br><div class="tags"><a href="/tags/李宏毅-机器学习/">李宏毅 机器学习</a></div><div class="post-nav"><a class="pre" href="/2019/06/03/「剑指Offer（第二版）」Python3实现/">「剑指Offer（第二版）」Python3实现</a><a class="next" href="/2018/12/27/「李宏毅机器学习」学习笔记-Unsupervised Learning - Transfer Learning/">「李宏毅机器学习」学习笔记-Transfer Learning</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://blueschang.github.io/2018/12/28/「李宏毅机器学习」学习笔记-Unsupervised Learning - Support Vector Machine/';
    this.page.identifier = '2018/12/28/「李宏毅机器学习」学习笔记-Unsupervised Learning - Support Vector Machine/';
    this.page.title = '「李宏毅机器学习」学习笔记-Support Vector Machine (SVM)';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//bluecode-1.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//bluecode-1.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://bluecode-1.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://blueschang.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Hexo-Github-博客/" style="font-size: 15px;">Hexo Github 博客</a> <a href="/tags/李宏毅-机器学习/" style="font-size: 15px;">李宏毅 机器学习</a> <a href="/tags/剑指Offer-Python-Algorithm/" style="font-size: 15px;">剑指Offer Python Algorithm</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/06/03/「剑指Offer（第二版）」Python3实现/">「剑指Offer（第二版）」Python3实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/28/「李宏毅机器学习」学习笔记-Unsupervised Learning - Support Vector Machine/">「李宏毅机器学习」学习笔记-Support Vector Machine (SVM)</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/27/「李宏毅机器学习」学习笔记-Unsupervised Learning - Transfer Learning/">「李宏毅机器学习」学习笔记-Transfer Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model Part II/">「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model (Part II)</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model Part I/">「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Generative Model (Part I)</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/26/「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Auto-encoder/">「李宏毅机器学习」学习笔记-Unsupervised Learning - Deep Auto-encoder</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/25/「李宏毅机器学习」学习笔记-Unsupervised Learning - Neighbor Embedding/">「李宏毅机器学习」学习笔记-Unsupervised Learning - Neighbor Embedding</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/25/「李宏毅机器学习」学习笔记-Unsupervised Learning - Word Embedding/">「李宏毅机器学习」学习笔记-Unsupervised Learning - Word Embedding</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/24/「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods/">「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/20/「李宏毅机器学习」学习笔记-Semi-supervised/">「李宏毅机器学习」学习笔记-Semi-supervised</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://toaco.github.io/" title="Jeffrey‘s Blog" target="_blank">Jeffrey‘s Blog</a><ul></ul><a href="https://coder-wen.github.io/" title="coderwen的踩坑日记" target="_blank">coderwen的踩坑日记</a><ul></ul><a href="https://me.csdn.net/taoyafan" title="taoyafan" target="_blank">taoyafan</a><ul></ul><a href="https://smilexnan.github.io/" title="SmileCode" target="_blank">SmileCode</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">BlueCode.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>