<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods | BlueCode</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-76546459-1','auto');ga('send','pageview');
</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods</h1><a id="logo" href="/.">BlueCode</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/donate/"><i class="fa fa-cny"> 赞助</i></a><a href="/app/"><i class="fa fa-adn"> 软件</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods</h1><div class="post-meta">Dec 24, 2018<span> | </span><span class="category"><a href="/categories/机器学习/">机器学习</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" data-disqus-identifier="2018/12/24/「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods/" href="/2018/12/24/「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Unsupervised-Learning"><span class="toc-number">1.</span> <span class="toc-text">Unsupervised Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Clustering"><span class="toc-number">2.</span> <span class="toc-text">Clustering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dimension-Reduction"><span class="toc-number">3.</span> <span class="toc-text">Dimension Reduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Principle-Component-Analysis-PCA"><span class="toc-number">4.</span> <span class="toc-text">Principle Component Analysis (PCA)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#数学推导"><span class="toc-number">4.1.</span> <span class="toc-text">数学推导</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PCA-–-Another-Point-of-View"><span class="toc-number">4.2.</span> <span class="toc-text">PCA – Another Point of View</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Weakness-of-PCA"><span class="toc-number">4.3.</span> <span class="toc-text">Weakness of PCA</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Matrix-Factorization"><span class="toc-number">5.</span> <span class="toc-text">Matrix Factorization</span></a></li></ol></div></div><div class="post-content"><p>本章课程<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/dim%20reduction%20%28v5%29.pdf" target="_blank" rel="noopener">PDF</a>，视频（<a href="https://www.youtube.com/watch?v=iwh5o_M4BNU&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&amp;index=22" target="_blank" rel="noopener">油管</a>或<a href="https://www.bilibili.com/video/av10590361/?p=24" target="_blank" rel="noopener">B站</a>）。</p>
<h3 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h3><p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi564ckz3j21fo0t2av7.jpg" alt=""></p>
<h3 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h3><p><img src="https://ws1.sinaimg.cn/large/d1566b02ly1fyi5dvgfxnj21e81261ge.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02ly1fyi5evnkujj219o0sswhj.jpg" alt=""></p>
<p>假设现在有5个example，先建一个树结构，两两之间计算相似度，然后挑最相似的一对出来，把这两个merge，得到一个新的向量，这个向量同时代表第一个和第二个example，现在有四个example，重新两两计算相似度，再选取最相似的两个，再把它们merge起来，然后继续这样操作，最后把红色和绿色merge起来，建立出一个树结构。接着，在树上切一刀，如果是切在红色线，就会把5个data分成2类，切在蓝色线，会把5个data分成3类，切在绿色线，会把5个data分成4类。</p>
<p>K-means与HAC的最大差别是如何决定cluster的数目，在K-means里面，需要决定k的数值是多少，HAC不用直接决定cluster的数目，只需要决定切在树的哪里。</p>
<h3 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h3><p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi5u2sbf6j21g40si7kx.jpg" alt=""></p>
<p>极端的例子，如数字3，可以只用旋转的度数来描述这5张图片。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi5v66gutj21900sk0zx.jpg" alt=""></p>
<p>Feature selection如果data point的某一维都不变，就没有存在意义，可以去掉。 此方法适用场合有限。</p>
<h3 id="Principle-Component-Analysis-PCA"><a href="#Principle-Component-Analysis-PCA" class="headerlink" title="Principle Component Analysis (PCA)"></a>Principle Component Analysis (PCA)</h3><p>希望新的特征的variance尽量大，所以k个w选择的是cov(x)的前k个最大的eigenvalue对应的eigenvector。 </p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi5x77o24j21by116n8t.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi5x70tm2j21dm0ysjxj.jpg" alt=""></p>
<h4 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h4><p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi5yfj9z6j21du11eagw.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi5yfhcdbj21dy11kn47.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi5yftlb9j21de116n4c.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi5yfoxy9j21ec1044at.jpg" alt=""></p>
<h4 id="PCA-–-Another-Point-of-View"><a href="#PCA-–-Another-Point-of-View" class="headerlink" title="PCA – Another Point of View"></a>PCA – Another Point of View</h4><p>用SVD方法得到PCA的解。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi64h2ahfj21fa0to45g.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi64hvnqrj21cs0uuq8p.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi64hdtj5j21b60zmq81.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi64hpsaqj21gk11oq7w.jpg" alt=""></p>
<p>从神经网络的角度理解PCA。</p>
<p>train这个神经网络的参数，让output与input越接近越好。<br>用gradient descent解出来的结果与PCA的解出来的$w$是不一样，不能保证$w^i$,$w^j$ 正交。 效果也不会比PCA的解更好。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi661zjydj21ew10wn6e.jpg" alt=""></p>
<h4 id="Weakness-of-PCA"><a href="#Weakness-of-PCA" class="headerlink" title="Weakness of PCA"></a>Weakness of PCA</h4><p>PCA是无监督的，不知道数据的标签，这样在降维映射之后可能会把两类数据混到一起。 考虑数据标签的方法LDA（Linear Discriminant Analysis）可以避免这一问题。</p>
<p>PCA是线性的。把一个三维空间中的S形分布的数据做PCA之后的效果，就是把S形拍扁，而非展开。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi6ekryh0j21di0vu7pl.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi6q8n1y0j21du0zy7bb.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi6q8zf9jj218w0ykgta.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi6q8heutj21by0vajvn.jpg" alt=""></p>
<h3 id="Matrix-Factorization"><a href="#Matrix-Factorization" class="headerlink" title="Matrix Factorization"></a>Matrix Factorization</h3><p>人买公仔，人和公仔背后都有共同的隐藏属性影响人买多少公仔，例如人的属性：萌傲娇/萌天然呆，公仔的属性：傲娇/天然呆。<br>我们要从购买记录（矩阵）中推断出latent factor，latent factor的数目需要事先定好。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi6sfkjaej21e210q1ag.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi6sf7v3gj21f413e1b2.jpg" alt=""></p>
<p>对矩阵做SVD，SVD的中间矩阵可以并到左边矩阵或右边矩阵。</p>
<p>有missing data怎么办？用gradient descent做，先定义loss function L（只考虑有数值的数据）。 </p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi6si381fj21ei11qk8c.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi6sih1mdj21ds11ydx5.jpg" alt=""></p>
<p>得到$r^A$,$r^B$,$r^C$,$r^D$,$r^E$,$r^1$,$r^2$,$r^3$,$r^4$之后，并不知道每个维度代表什么属性，需要事后分析。 </p>
<p>已知姐寺与小唯属于天然呆类型、春日与炮姐属于傲娇类型，所以第一个维度代表天然呆，第二个维度代表傲娇。</p>
<p><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi79zxf8fj21cu0ysgr8.jpg" alt=""></p>
<p> <img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi7c9uqp5j21bw108th4.jpg" alt=""><br><img src="https://ws1.sinaimg.cn/large/d1566b02gy1fyi7c9wmfsj219a0wu44z.jpg" alt=""></p>
<p>MDS特别的地方是它不需要把每一个data都表示成特征向量，只需要知道特征向量之间的距离。MDS和PCA是有一些特殊的关系，如果用某一些特定的距离来衡量两个data point之间的距离的话，做MDS就等于是做PCA，PCA保留了原来在高维空间中的距离，如果两个点在高维空间中的距离是远的，在低微的空间中，它的距离也是远的，在高维空间中是接近的，在低维空间中也是接近的。</p>
<p>PCA有几率的版本叫Probabilistic PCA。</p>
<p>PCA有非线性的版本叫Kernel PCA。</p>
<p>CCA是针对有两种不同的source，比如做语音辨识，有两个source，一个是声音讯号，一个是唇形的image，可以把这两种不同的source都做降维。</p>
<p>ICA里不找orthogonal的component，找independent component。</p>
<p>LDA是supervise的方式。</p>
<p><em>如果本博文对您有帮助，可以<a href="https://blueschang.github.io/donate/">赞助</a>支持一波博主~</em></p>
</div><div class="recommended_posts"><h3>推荐阅读</h3><li><a href="https://blueschang.github.io/2018/12/20/「李宏毅机器学习」学习笔记-Semi-supervised/" target="_blank">「李宏毅机器学习」学习笔记-Semi-supervised</a></li><li><a href="https://blueschang.github.io/2018/11/17/「李宏毅机器学习」学习笔记-Why Deep/" target="_blank">「李宏毅机器学习」学习笔记-Why Deep?</a></li><li><a href="https://blueschang.github.io/2018/11/15/「李宏毅机器学习」学习笔记-Convolutional Neural Network/" target="_blank">「李宏毅机器学习」学习笔记-Convolutional Neural Network</a></li><li><a href="https://blueschang.github.io/2018/11/09/「李宏毅机器学习」学习笔记-Tips for Training DNN/" target="_blank">「李宏毅机器学习」学习笔记-Tips for Training DNN</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>BluesChang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2018/12/24/「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods/">https://blueschang.github.io/2018/12/24/「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a> 许可协议。转载请注明出处！</li></ul></div><br><div class="tags"><a href="/tags/李宏毅-机器学习/">李宏毅 机器学习</a></div><div class="post-nav"><a class="next" href="/2018/12/20/「李宏毅机器学习」学习笔记-Semi-supervised/">「李宏毅机器学习」学习笔记-Semi-supervised</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://blueschang.github.io/2018/12/24/「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods/';
    this.page.identifier = '2018/12/24/「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods/';
    this.page.title = '「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//bluecode-1.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//bluecode-1.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://bluecode-1.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://blueschang.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Hexo-Github-博客/" style="font-size: 15px;">Hexo Github 博客</a> <a href="/tags/李宏毅-机器学习/" style="font-size: 15px;">李宏毅 机器学习</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/12/24/「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods/">「李宏毅机器学习」学习笔记-Unsupervised Learning - Linear Methods</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/20/「李宏毅机器学习」学习笔记-Semi-supervised/">「李宏毅机器学习」学习笔记-Semi-supervised</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/17/「李宏毅机器学习」学习笔记-Why Deep/">「李宏毅机器学习」学习笔记-Why Deep?</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/15/「李宏毅机器学习」学习笔记-Convolutional Neural Network/">「李宏毅机器学习」学习笔记-Convolutional Neural Network</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/09/「李宏毅机器学习」学习笔记-Tips for Training DNN/">「李宏毅机器学习」学习笔记-Tips for Training DNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/08/「李宏毅机器学习」学习笔记-“Hello world” of deep learning/">「李宏毅机器学习」学习笔记-“Hello world” of deep learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/08/「李宏毅机器学习」学习笔记-Backpropagation/">「李宏毅机器学习」学习笔记-Backpropagation</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/07/「李宏毅机器学习」学习笔记-Brief Introduction of Deep Learning/">「李宏毅机器学习」学习笔记-Brief Introduction of Deep Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/03/「李宏毅机器学习」学习笔记-Logistic Regression/">「李宏毅机器学习」学习笔记-Logistic Regression</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/02/「李宏毅机器学习」学习笔记-Classification/">「李宏毅机器学习」学习笔记-Classification：Probabilistic Generative Model</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://toaco.github.io/" title="Jeffrey‘s Blog" target="_blank">Jeffrey‘s Blog</a><ul></ul><a href="https://coder-wen.github.io/" title="coderwen的踩坑日记" target="_blank">coderwen的踩坑日记</a><ul></ul><a href="https://me.csdn.net/taoyafan" title="taoyafan" target="_blank">taoyafan</a><ul></ul><a href="https://smilexnan.github.io/" title="SmileCode" target="_blank">SmileCode</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">BlueCode.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>